# Documentation du Projet - DÃ©ploiement d'un ModÃ¨le de Machine Learning

# ğŸ™ï¸ PrÃ©diction de la Consommation d'Ã‰nergie des BÃ¢timents Ã  Seattle

Ce projet de Machine Learning vise Ã  prÃ©dire la consommation dâ€™Ã©nergie des bÃ¢timents non rÃ©sidentiels de Seattle, en se basant sur des donnÃ©es publiques de 2016.  
Lâ€™objectif est dâ€™**industrialiser un modÃ¨le** en le structurant dans un projet Python versionnÃ© avec Git, prÃªt Ã  Ãªtre exÃ©cutÃ©, Ã©valuÃ©, maintenu et dÃ©ployÃ© via CI/CD.

### 1. PrÃ©sentation GÃ©nÃ©rale

Ce dÃ©pÃ´t contient lâ€™ensemble du code source et des ressources nÃ©cessaires pour entraÃ®ner, Ã©valuer, dÃ©ployer et maintenir un modÃ¨le de machine learning Ã  lâ€™aide de FastAPI, PostgreSQL, Gradio, et GitHub Actions (CI/CD).

---

## Installation Locale

```bash
git clone https://github.com/Vagaboss/deploiement_machine_learning_model.git
cd gitP3
pip install -r requirements.txt
```
## Historique des commit et tag
- git log
- git tag

---

## ğŸ“¦ DonnÃ©es utilisÃ©es

Le fichier `2016_Building_Energy_Benchmarking.csv` doit Ãªtre placÃ© dans le dossier `data/`.
Le fichier `cleaned_data.csv` doit Ãªtre placÃ© dans le dossier `data/`. C'est le dataset nettoyÃ© suite au preprocessing.

ğŸ”— Ils contiennent des informations principales sur les bÃ¢timents de Seattle (surface, type, annÃ©e, usage, consommation...).

---

### 2. Structure du DÃ©pÃ´t Git 

Le modÃ¨le de machine learning, initialement dÃ©veloppÃ© dans un notebook, a Ã©tÃ© migrÃ© dans une architecture modulaire avec des fichiers `.py` pour assurer la maintenabilitÃ© 

ğŸ“¦ deploiement_machine_learning_model/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ test-model.yaml
â”œâ”€â”€ DB schema/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ schemas.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned_data.csv
â”‚   â””â”€â”€ 2016_Building_Energy_Benchmarking.csv
â”œâ”€â”€ hf_spaces/
â”‚   â””â”€â”€ P3_ML_deployment/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ p3deplo(1).ipynb
â”œâ”€â”€ src/
â”‚   â””â”€â”€ preprocessing.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â””â”€â”€ test_train_model.py
â”œâ”€â”€ .coverage
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ README_CLEAN_FINAL.md
â”œâ”€â”€ create_db.py
â”œâ”€â”€ evaluate_model.py
â”œâ”€â”€ insert_data.py
â”œâ”€â”€ insert_outputs.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ train_model.py


---

## Lancer l'entrainement et Ã©valuation du modÃ¨le (commandes GIT)
- python train_model.py
   - Nettoie, encode et standardise les donnÃ©es
   - EntraÃ®ne un `RandomForestRegressor` avec `GridSearchCV`
   - Enregistre le modÃ¨le dans `models/best_rf_pipeline.joblib`

- python evaluate_model.py
  - Recharge le modÃ¨le
  - Affiche les performances :
    - RÂ² (coefficient de dÃ©termination)
    - MAE (erreur absolue moyenne)
    - RMSE (racine de l'erreur quadratique moyenne)

### 3. CI/CD avec GitHub Actions

## IntÃ©gration Continue (CI)

Une action GitHub dÃ©clenchÃ©e Ã  chaque `push` ou `pull_request` sur les branches `main` et `dev` :

- Installe les dÃ©pendances (`requirements.txt`)
- EntraÃ®ne et Ã©value le modÃ¨le

ğŸ“„ Fichier : `.github/workflows/test-model.yml`

## DÃ©ploiement Continu (CD)

Le dÃ©ploiement vers Hugging Face Spaces est automatisÃ© :
- Le workflow push automatiquement le contenu du dossier `hf_spaces/` vers lâ€™espace distant.
- Lâ€™interface Gradio est ainsi mise Ã  jour dÃ¨s que le modÃ¨le est entraÃ®nÃ©.

ğŸ“„ Fichier : `.github/workflows/test-model.yml`
---

### 4. Interface Hugging Face (Gradio)

Une interface utilisateur intuitive a Ã©tÃ© dÃ©veloppÃ©e avec Gradio pour permettre de tester le modÃ¨le :

- Saisir les caractÃ©ristiques du bÃ¢timent
- Obtenir la prÃ©diction de consommation Ã©nergÃ©tique (en kBtu)
- Accessible Ã  : [https://huggingface.co/spaces/yacineould/P3_ML_deployment](https://huggingface.co/spaces/yacineould/P3_ML_deployment)

---

## ğŸŒ Exemple dâ€™utilisation â€“ Hugging Face Spaces
ğŸ¯ Objectif : faire une prÃ©diction via lâ€™interface Gradio en ligne

    - AccÃ©der Ã  lâ€™espace dÃ©ployÃ©

    - Utiliser lâ€™interface graphique

    - Renseigne les valeurs des champs affichÃ©s (type de bÃ¢timent, nombre d'Ã©tages, etc.)

    - Clique sur â€œPrÃ©direâ€

    RÃ©sultat affichÃ© :

    - Une prÃ©diction s'affiche dans un encadrÃ©, indiquant la consommation prÃ©dite.

### 5. API FastAPI

Lâ€™API expose plusieurs endpoints documentÃ©s via Swagger :

- `/predict` : prÃ©diction Ã  partir des donnÃ©es utilisateurs
- `/health` : vÃ©rifie si lâ€™API fonctionne
- `/history` : historique des requÃªtes
- `/dataset` : visualisation dâ€™un extrait du dataset

Documentation interactive disponible Ã  : `http://127.0.0.1:8000/docs`

- Ouvrir la base de donnÃ©es PostgreSQL "build-seatle"
- Se mettre dans gitP3 et lancer la commande git : uvicorn api.main:app
- Ouvrir http://127.0.0.1:8000/docs 
- Vous pourrez tester lâ€™API via Swagger.
---
## ğŸŒ Exemple dâ€™utilisation â€“ FASTAPI
- A rentrer dans l'endpoint "Predict"
- Cliquer sur "Try it out"
- Exemple : 
{
  "PropertyGFATotal": 1000000,
  "NumberofFloors": 4,
  "NumberofBuildings": 2,
  "YearBuilt": 2010,
  "HasGas": true,
  "HasElectricity": true,
  "HasSteam": true,
  "HasParking": true,
  "UsageCount": "string",
  "PropertyTypeGrouped": "string",
  "PrimaryPropertyType": "string",
  "Neighborhood": "string"
}
- Appuyer sur Execute
- La valeur prÃ©dite est visible juste en dessous


### 6. Base de DonnÃ©es PostgreSQL

Une base PostgreSQL ("build-seatle") hÃ©berge :

- Le dataset complet
- Les entrÃ©es (`inputs`) et sorties (`outputs`) des requÃªtes utilisateur effectuÃ©es dans l'API

## Connexion via SQLAlchemy

La base est reliÃ©e Ã  lâ€™API FastAPI pour historiser les interactions.

---

### 7. Tests Unitaires et Fonctionnels

Trois types de tests ont Ã©tÃ© rÃ©alisÃ©s avec Pytest :

- **Preprocessing** : `pytest tests/test_preprocessing.py`
- **ModÃ¨le ML** : `pytest tests/test_train_model.py`
- **API** : `pytest tests/test_api.py`

Les tests couvrent les cas critiques et les erreurs. Des rapports de couverture ont Ã©tÃ© gÃ©nÃ©rÃ©s.

## Comment les lancer ? 

- Se mettre dans gitP3
- effectuer les commandes git suivantes : 
  - pytest tests/test_preprocessing.py
  - pytest tests/test_train_model.py
  - pytest tests/test_api.py

---
### 8. Voir les rapports de couverture

 Il faut se rendre dans le sous dossier htmlcov
 
 - Cliquer sur les 2 fichiers commenÃ§ant par "Couverture"


### 9. Protocole de Maintenance

- **Mise Ã  jour du modÃ¨le** : rÃ©entraÃ®nement ou mise Ã  jour via modification du `train_model.py`
- **Mise Ã  jour de l'API** : mise Ã  jour des fichier .py dans le sous dossier "api"
- **DÃ©ploiement auto** : push sur `main` ou `dev` dÃ©clenche le workflow CI/CD
- **Secrets** : gÃ©rÃ©s via `HF_TOKEN` dans GitHub

---

### 10. Justification des Choix Techniques

- **FastAPI** : rapiditÃ© de dÃ©veloppement, documentation automatique
- **PostgreSQL** : robustesse, requÃªtes complexes
- **Gradio** : interface rapide Ã  dÃ©ployer
- **GitHub Actions** : intÃ©gration fluide avec GitHub

---

### 11. ğŸ“Œ Documentation technique du modÃ¨le de Machine Learning

## Architecture du modÃ¨le

Le modÃ¨le est un pipeline scikit-learn entraÃ®nÃ© sur un dataset de consommation Ã©nergÃ©tique des bÃ¢timents. Il est composÃ© de :

    Un prÃ©processing (encodage, gestion des valeurs manquantes, normalisation)

    Un estimateur final 

    Ce pipeline est sauvegardÃ© avec joblib pour Ãªtre utilisÃ© dans lâ€™API ou dans lâ€™espace Hugging Face.


- train_model.py	: EntraÃ®ne et sauvegarde le modÃ¨le ML
- evaluate_model.py	: Ã‰value les performances sur un jeu de test (MAE, RMSE, RÂ²)
- models/best_rf_pipeline.joblib	: Contient le pipeline entraÃ®nÃ©, prÃªt Ã  Ãªtre utilisÃ© par lâ€™API

## Pour entraÃ®ner et Ã©valuer le modÃ¨le :

python train_model.py
python evaluate_model.py

- Performances du modÃ¨le (exemple)
MÃ©trique	Score
RMSE	12.8
MAE	9.5
RÂ²	0.78

    Ces rÃ©sultats sont indicatifs et peuvent varier selon la version du dataset ou les hyperparamÃ¨tres.

### 12. ğŸ”„ Maintenance et mise Ã  jour du modÃ¨le et de lâ€™API

## Mettre Ã  jour le modÃ¨le

    Modifier ou remplacer le dataset ou la logique dans train_model.py

    RÃ©entraÃ®ner :

python train_model.py
python evaluate_model.py

- VÃ©rifier que le fichier best_rf_pipeline.joblib a bien Ã©tÃ© mis Ã  jour

- Commiter et pousser :

    git add models/best_rf_pipeline.joblib
    git commit -m "ğŸ”„ Update modÃ¨le ML"
    git push

    Le dÃ©ploiement vers Hugging Face sera automatique grÃ¢ce Ã  la CD configurÃ©e avec GitHub Actions

## Mettre Ã  jour lâ€™API FastAPI

    Modifier la logique dans api/main.py ou les dÃ©pendances associÃ©es (schemas.py, db.py, etc.)

    Tester localement :

uvicorn api.main:app --reload

- Si tout fonctionne :

    git add .
    git commit -m "âœ¨ Mise Ã  jour API"
    git push

    âš ï¸ Attention : lâ€™API nâ€™est pas dÃ©ployÃ©e automatiquement. Elle tourne localement ou doit Ãªtre dÃ©ployÃ©e manuellement (Render, Railway, etc.)

### 13. Protocole de Mise Ã  jour 

ğŸ” Protocole de mise Ã  jour rÃ©guliÃ¨re

Pour assurer la pÃ©rennitÃ© et la fiabilitÃ© du projet, voici un protocole de mise Ã  jour structurÃ©, Ã  suivre tous les mois (ou Ã  chaque Ã©volution majeure) :

1. Mise Ã  jour des donnÃ©es

    ğŸ“… FrÃ©quence : mensuelle ou dÃ¨s quâ€™un nouveau dataset est disponible.

    ğŸ”§ Action : insÃ©rer les nouvelles donnÃ©es dans la base PostgreSQL (via psql ou script Python).

    âš ï¸ Impact : nÃ©cessite potentiellement un rÃ©entraÃ®nement du modÃ¨le.

2. RÃ©entraÃ®nement du modÃ¨le

    ğŸ“… FrÃ©quence : Ã  chaque mise Ã  jour significative des donnÃ©es ou dÃ©gradation des performances.

    âš™ï¸ Action :

    python train_model.py
    python evaluate_model.py

    âœ… Ã‰valuation : valider que les nouvelles performances sont meilleures ou stables.

    ğŸ’¾ Sauvegarde : remplacer le modÃ¨le enregistrÃ© dans models/best_rf_pipeline.joblib.

3. Mise Ã  jour de lâ€™API

    ğŸ” Si la structure du modÃ¨le change (nouveaux inputs ou changement de format), mettre Ã  jour :

        Le schÃ©ma Pydantic dans main.py

        Les endpoints concernÃ©s

        Les tables inputs et outputs de la base PostgreSQL

    Tester localement avec :

    uvicorn api.main:app --reload

4. Mise Ã  jour de lâ€™espace Hugging Face

    ğŸ“¦ Remplacer le fichier modÃ¨le dans hf_spaces/P3_ML_deployment/models/

    âš™ï¸ La CI/CD dÃ©clenche automatiquement un nouveau dÃ©ploiement via GitHub Actions

    ğŸ¯ VÃ©rifier le bon fonctionnement de lâ€™interface Gradio

5. VÃ©rification des tests

    ğŸ§ª Relancer tous les tests :

    pytest tests/test_preprocessing.py
    pytest tests/test_train_model.py
    pytest tests/test_api.py

    ğŸ” Sâ€™assurer quâ€™ils passent tous Ã  100% avec pytest-cov

6. Mise Ã  jour de la documentation

    ğŸ“˜ Mettre Ã  jour le README.md :

        Nouvelles instructions dâ€™installation si besoin

        Changement dans les inputs, outputs, ou modÃ¨le

        Ajout de nouveaux exemples dâ€™utilisation

7. Versioning

    ğŸ“Œ Utiliser des tags Git pour marquer les nouvelles versions stables :

git tag -a v1.1 -m "Nouveau modÃ¨le + API mise Ã  jour"
git push origin v1.1

